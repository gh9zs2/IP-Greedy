{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import h5py\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id =3 #0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_MFdata():\n",
    "    file_name = '../dataset/dataset_MF200/'\n",
    "    if input_id ==0:\n",
    "        file_name += \"netflix_mf-200.txt\"\n",
    "    elif input_id ==1:\n",
    "        file_name += \"amazon_Movies_and_TV_mf-200.txt\"\n",
    "    elif input_id ==2:\n",
    "        file_name += \"amazon_Kindle_Store_mf-200.txt\"\n",
    "    elif input_id ==3:\n",
    "        file_name += \"MovieLens_mf-200.txt\"\n",
    "        \n",
    "    list_p = []\n",
    "    list_q = []\n",
    "    item_id_list = []\n",
    "    \n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()  \n",
    "            line_tmp = line.split() \n",
    "            if line_tmp[1] ==\"T\":\n",
    "                if line_tmp[0][0] == \"p\":\n",
    "                    del line_tmp[0:2]\n",
    "                    for i in range(len(line_tmp)):\n",
    "                        line_tmp[i] = float(line_tmp[i])\n",
    "                    list_p.append(line_tmp)\n",
    "                    \n",
    "                elif line_tmp[0][0] == \"q\":\n",
    "                    item_id_list.append(int(line_tmp[0][1:]))\n",
    "                    del line_tmp[0:2]\n",
    "                    for i in range(len(line_tmp)):\n",
    "                        line_tmp[i] = float(line_tmp[i])\n",
    "                    list_q.append(line_tmp)\n",
    "                \n",
    "    data_user = np.array(list_p)\n",
    "    data_item = np.array(list_q)\n",
    "    return data_user, data_item, item_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_i2v_data():\n",
    "    file_name = '../dataset/dataset_item2vec/'\n",
    "    if input_id ==0:\n",
    "        file_name += \"netflix_item2vec_d-200.txt\"\n",
    "    elif input_id ==1:\n",
    "        file_name += \"amazon_Movie_item2vec_d-200.txt\"\n",
    "    elif input_id ==2:\n",
    "        file_name += \"amazon_Kindle_item2vec_d-200.txt\"\n",
    "    elif input_id ==3:\n",
    "        file_name += \"MovieLens_item2vec_d-200.txt\"\n",
    "        \n",
    "    item_data = []\n",
    "    \n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()  \n",
    "            line_tmp = line.split() \n",
    "            del line_tmp[0]\n",
    "            for i in range(len(line_tmp)):\n",
    "                line_tmp[i] = float(line_tmp[i])\n",
    "            item_data.append(line_tmp)\n",
    "\n",
    "    \n",
    "    return np.array(item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_para():\n",
    "    file_name = '../parameter/k.txt'\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()  \n",
    "            line_tmp = line.split() \n",
    "            k = int(line_tmp[0])\n",
    "            \n",
    "    file_name = '../parameter/lamda.txt'\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()  \n",
    "            line_tmp = line.split() \n",
    "            lamda = float(line_tmp[0])\n",
    "\n",
    "    \n",
    "    return k, lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_forcus_users():\n",
    "    file_name = './forcus_users/'\n",
    "    if input_id ==0:\n",
    "        file_name += \"netflix.csv\"\n",
    "    elif input_id ==1:\n",
    "        file_name += \"amazon_M.csv\"\n",
    "    elif input_id ==2:\n",
    "        file_name += \"amazon_K.csv\"\n",
    "    elif input_id ==3:\n",
    "        file_name += \"MovieLens.csv\"\n",
    "        \n",
    "    forcus_user = []\n",
    "    \n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()  \n",
    "            line_tmp = line.split(\",\") \n",
    "            forcus_user.append(int(float(line_tmp[0])))\n",
    "          \n",
    "    return forcus_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scale(data):\n",
    "    max_scalar_vec = np.max(data, axis=0)\n",
    "    min_scalar_vec = np.min(data, axis=0)\n",
    "\n",
    "    tmp_list = []\n",
    "    for i in range(data.shape[1]):\n",
    "        tmp_list.append(max_scalar_vec[i] - min_scalar_vec[i])\n",
    "        \n",
    "    X = np.array(tmp_list)\n",
    "    return np.linalg.norm(X, ord=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_min_dist(data, index_vec):\n",
    "    dist_list = []\n",
    "    for i in range(len(index_vec)):\n",
    "        left = index_vec[i]\n",
    "        for j in range(i+1, len(index_vec)):\n",
    "            right = index_vec[j]\n",
    "            dist_list.append(np.linalg.norm(data[left]-data[right]))\n",
    "            \n",
    "    return min(dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input MF data\n",
    "user_data, item_mf_data, item_id_list = Read_MFdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input i2v data\n",
    "item_i2v_data= Read_i2v_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameters\n",
    "k, lamda = Read_para()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input forcus user\n",
    "forcus_user_list = Read_forcus_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute scale\n",
    "scale = get_scale(item_i2v_data)\n",
    "print(scale)\n",
    "scale = float(5/scale)\n",
    "print(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "tmp_leaves = int(math.sqrt(item_mf_data.shape[0]))\n",
    "tmp_leaves_search =int(tmp_leaves/10) \n",
    "train_number = 2500\n",
    "print(tmp_leaves)\n",
    "print(tmp_leaves_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = scann.scann_ops_pybind.builder(item_mf_data, 10, \"dot_product\").tree(\n",
    "    num_leaves= tmp_leaves, num_leaves_to_search= tmp_leaves_search, training_sample_size= train_number).score_ah(\n",
    "    2, anisotropic_quantization_threshold=0.2).build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_id==0:\n",
    "    os.makedirs('.scann_artefacts/netflix/wo1/', exist_ok=True)\n",
    "    # serialize the searcher\n",
    "    searcher.serialize('.scann_artefacts/netflix/wo1/')\n",
    "elif input_id==1:\n",
    "    os.makedirs('.scann_artefacts/amazon_M/wo1/', exist_ok=True) \n",
    "    searcher.serialize('.scann_artefacts/amazon_M/wo1/') \n",
    "elif input_id==2:\n",
    "    os.makedirs('.scann_artefacts/amazon_K/wo/', exist_ok=True)\n",
    "    searcher.serialize('.scann_artefacts/amazon_K/wo/') \n",
    "elif input_id==3:\n",
    "    os.makedirs('.scann_artefacts/MovieLens/wo/', exist_ok=True)\n",
    "    searcher.serialize('.scann_artefacts/MovieLens/wo/') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_id = []\n",
    "result_index = []\n",
    "result_time =[]\n",
    "result_dist_min = []\n",
    "result_ip_sum = []\n",
    "result_score = []\n",
    "ip_list =[]\n",
    "for i in range(len(forcus_user_list)):\n",
    "    query_id = forcus_user_list[i]\n",
    "    \n",
    "    start = time.time()\n",
    "    neighbors, distances = searcher.search(user_data[query_id], final_num_neighbors= k)\n",
    "    end = time.time()\n",
    "    \n",
    "    ip=0\n",
    "    for j in range(k): \n",
    "        ip_list.append(np.dot(user_data[query_id], item_mf_data[neighbors[j]]) )\n",
    "        ip += np.dot(user_data[query_id], item_mf_data[neighbors[j]]) \n",
    "    result_ip_sum.append(ip)\n",
    "    result_index.append(neighbors)\n",
    "    result_time.append(1000*(end - start))\n",
    "\n",
    "#get true item_id\n",
    "for i in range (len(result_index)):\n",
    "    tmp = []\n",
    "    for j in range(len(result_index[i])):\n",
    "        tmp.append( item_id_list[result_index[i][j]] )\n",
    "    result_id.append(tmp)\n",
    "\n",
    "#get min_dist\n",
    "for i in range(len(forcus_user_list)):\n",
    "    result_dist_min.append( compute_min_dist(item_i2v_data, result_index[i]))\n",
    "    \n",
    "#get score\n",
    "for i in range(len(forcus_user_list)):\n",
    "    tmp = float((lamda * result_ip_sum[i])/k) + scale * (1-lamda) * result_dist_min[i]\n",
    "    result_score.append(tmp)\n",
    "    \n",
    "#get result\n",
    "result = []\n",
    "for i in range(len(forcus_user_list)):\n",
    "    tmp =[]\n",
    "    tmp.append(forcus_user_list[i])\n",
    "    tmp.append(result_score[i])\n",
    "    #tmp.append(result_ip_sum[i])\n",
    "    tmp.append(result_dist_min[i])\n",
    "    tmp.append(result_time[i])\n",
    "    tmp.extend(result_id[i])\n",
    "    result.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Output_Result(data):\n",
    "    file_name = \"./\"\n",
    "    if input_id ==0:\n",
    "        file_name += \"./netflix200_k-\" + str(k) + \"_lam-\" + str(lamda) + \".csv\"\n",
    "    elif input_id ==1:\n",
    "        file_name += \"./amazon_M200_k\" + str(k) + \"_lam-\" + str(lamda) + \".csv\"\n",
    "    elif input_id ==2:\n",
    "        file_name += \"./amazon_K200_k\" + str(k) + \"_lam-\" + str(lamda) + \".csv\"\n",
    "    elif input_id ==3:\n",
    "        file_name += \"./MovieLens200_k\" + str(k) + \"_lam-\" + str(lamda) + \".csv\"\n",
    "        \n",
    "    with open(file_name, 'w', newline= '') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_Result(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}